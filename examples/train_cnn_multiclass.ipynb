{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d16f490",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41e263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from seaborn) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c17ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (2023.4.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from torch) (2023.4.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marinedenolle/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, filelock, torch\n",
      "Installing collected packages: mpmath, sympy, filelock, torch\n",
      "Successfully installed filelock-3.20.0 mpmath-1.3.0 sympy-1.14.0 torch-2.9.1\n",
      "Successfully installed filelock-3.20.0 mpmath-1.3.0 sympy-1.14.0 torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5f5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Add parent directory to path to import model\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from src.models.cnn import SeismicCNN, CompactSeismicCNN\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"\\n✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66638595",
   "metadata": {},
   "source": [
    "## Load Labeled Data\n",
    "\n",
    "Load the windowed seismograms and labels created by the multi-class labeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6b9820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:\n",
      "  Waveforms: windowed_waveforms_20251210_170458.npy\n",
      "  Labels: labels_20251210_170458.npy\n",
      "  Metadata: metadata_20251210_170458.csv\n",
      "\n",
      "✓ Data loaded successfully!\n",
      "  X shape: (719, 500)\n",
      "  y shape: (719,)\n",
      "\n",
      "Class distribution:\n",
      "  Noise (class 0): 300 samples (41.7%)\n",
      "  Traffic (class 1): 18 samples (2.5%)\n",
      "  Earthquake (class 2): 401 samples (55.8%)\n"
     ]
    }
   ],
   "source": [
    "# Find the most recent labeled dataset\n",
    "data_dir = Path(\"labeled_data\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory '{data_dir}' not found. Run multi_class_labeling.ipynb first.\")\n",
    "\n",
    "# Find most recent files\n",
    "waveform_files = sorted(data_dir.glob(\"windowed_waveforms_*.npy\"))\n",
    "label_files = sorted(data_dir.glob(\"labels_*.npy\"))\n",
    "metadata_files = sorted(data_dir.glob(\"metadata_*.csv\"))\n",
    "\n",
    "if not waveform_files or not label_files:\n",
    "    raise FileNotFoundError(\"No labeled data found. Run multi_class_labeling.ipynb first.\")\n",
    "\n",
    "# Use most recent files\n",
    "waveforms_file = waveform_files[-1]\n",
    "labels_file = label_files[-1]\n",
    "metadata_file = metadata_files[-1] if metadata_files else None\n",
    "\n",
    "print(f\"Loading data from:\")\n",
    "print(f\"  Waveforms: {waveforms_file.name}\")\n",
    "print(f\"  Labels: {labels_file.name}\")\n",
    "if metadata_file:\n",
    "    print(f\"  Metadata: {metadata_file.name}\")\n",
    "\n",
    "# Load data\n",
    "X = np.load(waveforms_file)  # Shape: (n_samples, window_length)\n",
    "y = np.load(labels_file)     # Shape: (n_samples,)\n",
    "\n",
    "if metadata_file:\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "else:\n",
    "    metadata = None\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "\n",
    "# Print class distribution\n",
    "class_names = ['Noise', 'Traffic', 'Earthquake']\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {class_names[int(label)]} (class {int(label)}): {count} samples ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d3420",
   "metadata": {},
   "source": [
    "## Prepare Data for Training\n",
    "\n",
    "Split data into train/validation/test sets and create PyTorch DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906cf1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X: (719, 1, 500)\n",
      "\n",
      "Data split:\n",
      "  Train: 503 samples (70.0%)\n",
      "  Val:   108 samples (15.0%)\n",
      "  Test:  108 samples (15.0%)\n",
      "\n",
      "Train class distribution:\n",
      "  Noise: 210 (41.7%)\n",
      "  Traffic: 13 (2.6%)\n",
      "  Earthquake: 280 (55.7%)\n",
      "\n",
      "Val class distribution:\n",
      "  Noise: 45 (41.7%)\n",
      "  Traffic: 2 (1.9%)\n",
      "  Earthquake: 61 (56.5%)\n",
      "\n",
      "Test class distribution:\n",
      "  Noise: 45 (41.7%)\n",
      "  Traffic: 3 (2.8%)\n",
      "  Earthquake: 60 (55.6%)\n",
      "\n",
      "✓ DataLoaders created with batch size: 32\n",
      "  Train batches: 16\n",
      "  Val batches: 4\n",
      "  Test batches: 4\n"
     ]
    }
   ],
   "source": [
    "# Add channel dimension for CNN: (n_samples, window_length) -> (n_samples, 1, window_length)\n",
    "X = X[:, np.newaxis, :]  # Add channel dimension\n",
    "\n",
    "print(f\"Reshaped X: {X.shape}\")\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Val:   {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Print class distribution per split\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    print(f\"\\n{split_name} class distribution:\")\n",
    "    unique, counts = np.unique(y_split, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  {class_names[int(label)]}: {count} ({count/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created with batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a06d9",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "Create the CNN model for 3-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9bcc424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration:\n",
      "  Input channels: 1\n",
      "  Input length: 500 samples\n",
      "  Number of classes: 3\n",
      "\n",
      "✓ Compact model created\n",
      "  Total trainable parameters: 9,347\n",
      "\n",
      "Model architecture:\n",
      "CompactSeismicCNN(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "num_classes = 3\n",
    "input_channels = 1  # Single channel (vertical component)\n",
    "input_length = X_train.shape[2]  # Window length in samples\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Input channels: {input_channels}\")\n",
    "print(f\"  Input length: {input_length} samples\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "\n",
    "# Choose model type: 'standard' or 'compact'\n",
    "model_type = 'compact'  # Use compact model for faster training\n",
    "\n",
    "if model_type == 'standard':\n",
    "    model = SeismicCNN(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_length=input_length,\n",
    "        dropout_rate=0.3\n",
    "    )\n",
    "else:\n",
    "    model = CompactSeismicCNN(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_length=input_length\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n✓ {model_type.capitalize()} model created\")\n",
    "print(f\"  Total trainable parameters: {n_params:,}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c554fc2",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Set up loss function, optimizer, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f279ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Learning rate scheduler\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining configuration:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Loss function (with class weights for imbalanced data)\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)  # Normalize\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler - simple step decay every 15 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  LR decay: 0.5x every 15 epochs\")\n",
    "print(f\"  Weight decay: {weight_decay}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"\\nClass weights:\")\n",
    "for i, (name, weight) in enumerate(zip(class_names, class_weights)):\n",
    "    print(f\"  {name}: {weight:.3f}\")\n",
    "print(f\"\\n✓ Training configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a8037",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35648f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average training loss\n",
    "        accuracy: Training accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average validation loss\n",
    "        accuracy: Validation accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43b377",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Train the model and track training/validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a98f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\\n\")\n",
    "print(f\"{'Epoch':<6} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12} {'Time':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate (call after each epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Print progress\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"{epoch+1:<6} {train_loss:<12.4f} {train_acc:<12.2f} {val_loss:<12.4f} {val_acc:<12.2f} {epoch_time:<8.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ Training complete!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final train accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"  Final val accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\n✓ Best model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6123d",
   "metadata": {},
   "source": [
    "## Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6e3c",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_proba = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_proba.extend(probabilities.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_proba = np.array(y_proba)\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"  Overall Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca062c",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n",
    "            yticklabels=class_names, ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('True', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names,\n",
    "            yticklabels=class_names, ax=axes[1], cbar_kws={'label': 'Proportion'})\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('True', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdc9d4",
   "metadata": {},
   "source": [
    "## Visualize Predictions on Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random examples from test set\n",
    "n_examples = 9\n",
    "random_indices = np.random.choice(len(X_test), n_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get waveform\n",
    "    waveform = X_test[idx, 0, :]  # Remove channel dimension\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    proba = y_proba[idx]\n",
    "    \n",
    "    # Plot waveform\n",
    "    time_axis = np.arange(len(waveform)) / 100.0  # Assuming 100 Hz\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.plot(time_axis, waveform, color=color, linewidth=1)\n",
    "    \n",
    "    # Title with prediction info\n",
    "    title = f\"True: {class_names[true_label]}, Pred: {class_names[pred_label]}\\n\"\n",
    "    title += f\"Conf: {proba[pred_label]*100:.1f}%\"\n",
    "    ax.set_title(title, fontsize=10, color=color)\n",
    "    ax.set_xlabel('Time (s)', fontsize=9)\n",
    "    ax.set_ylabel('Amplitude', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Test Set Predictions (Green=Correct, Red=Incorrect)', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Example predictions visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e467e",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65513d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"seismic_cnn_{model_type}_{timestamp}.pth\"\n",
    "model_path = models_dir / model_filename\n",
    "\n",
    "# Save model state dict and metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_type': model_type,\n",
    "    'num_classes': num_classes,\n",
    "    'input_channels': input_channels,\n",
    "    'input_length': input_length,\n",
    "    'class_names': class_names,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'history': history,\n",
    "    'training_config': {\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'weight_decay': weight_decay\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  Type: {model_type}\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Input shape: (batch, {input_channels}, {input_length})\")\n",
    "print(f\"  Output classes: {num_classes}\")\n",
    "\n",
    "# Save training summary\n",
    "summary_file = models_dir / f\"training_summary_{timestamp}.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"Seismic CNN Training Summary\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Model Configuration:\\n\")\n",
    "    f.write(f\"  Type: {model_type}\\n\")\n",
    "    f.write(f\"  Parameters: {n_params:,}\\n\")\n",
    "    f.write(f\"  Input shape: (batch, {input_channels}, {input_length})\\n\")\n",
    "    f.write(f\"  Number of classes: {num_classes}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training Configuration:\\n\")\n",
    "    f.write(f\"  Epochs: {num_epochs}\\n\")\n",
    "    f.write(f\"  Batch size: {batch_size}\\n\")\n",
    "    f.write(f\"  Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"  Weight decay: {weight_decay}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Dataset Split:\\n\")\n",
    "    f.write(f\"  Train: {len(X_train)} samples\\n\")\n",
    "    f.write(f\"  Validation: {len(X_val)} samples\\n\")\n",
    "    f.write(f\"  Test: {len(X_test)} samples\\n\\n\")\n",
    "    \n",
    "    f.write(\"Results:\\n\")\n",
    "    f.write(f\"  Best validation loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(f\"  Final train accuracy: {history['train_acc'][-1]:.2f}%\\n\")\n",
    "    f.write(f\"  Final val accuracy: {history['val_acc'][-1]:.2f}%\\n\")\n",
    "    f.write(f\"  Test accuracy: {test_accuracy*100:.2f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "print(f\"\\n✓ Training summary saved to: {summary_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noisepy-deploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
