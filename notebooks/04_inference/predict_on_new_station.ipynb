{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215ed9b7",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "**Customize these parameters for your analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Target station: AM.RB38A\n",
      "  Window: 5.0s with 50% overlap\n",
      "  Confidence threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - CUSTOMIZE THESE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Station information\n",
    "NETWORK = \"AM\"  # Network code\n",
    "STATION = \"RB38A\"  # Station code to analyze (e.g., 'R4017', 'RB38A', 'R1796')\n",
    "CHANNEL = \"EHZ\"  # Channel (vertical component)\n",
    "\n",
    "# Time window for analysis\n",
    "START_TIME = \"2024-11-27T17:00:00\"  # UTC time (YYYY-MM-DDTHH:MM:SS)\n",
    "END_TIME = \"2024-11-27T17:30:00\"    # UTC time (30 minutes)\n",
    "\n",
    "# Model parameters (should match training)\n",
    "WINDOW_LENGTH_SEC = 5.0  # Window length in seconds\n",
    "WINDOW_OVERLAP = 0.5  # 50% overlap\n",
    "\n",
    "# Visualization parameters\n",
    "CONFIDENCE_THRESHOLD = 0.5  # Minimum confidence for positive detection\n",
    "N_EXAMPLE_WINDOWS = 12  # Number of example windows to display\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Target station: {NETWORK}.{STATION}\")\n",
    "print(f\"  Time window: {START_TIME} to {END_TIME}\")\n",
    "print(f\"  Window: {WINDOW_LENGTH_SEC}s with {WINDOW_OVERLAP*100:.0f}% overlap\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c9a4e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ObsPy imports\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from src.models.cnn import SeismicCNN, CompactSeismicCNN\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"\\n✓ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4950ed",
   "metadata": {},
   "source": [
    "## Download Seismic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for earthquakes M>=4.0 in region...\n"
     ]
    },
    {
     "ename": "FDSNNoDataException",
     "evalue": "No data available for request.\nHTTP Status code: 204\nDetailed response of server:\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFDSNNoDataException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_EARTHQUAKE_TIME:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching for earthquakes M>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEARTHQUAKE_MIN_MAG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in region...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     catalog \u001b[38;5;241m=\u001b[39m \u001b[43miris_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUTCDateTime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUTCDateTime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMIN_LAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMIN_LON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminmagnitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEARTHQUAKE_MIN_MAG\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(catalog) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo earthquakes found with M>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEARTHQUAKE_MIN_MAG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:550\u001b[0m, in \u001b[0;36mClient.get_events\u001b[0;34m(self, starttime, endtime, minlatitude, maxlatitude, minlongitude, maxlongitude, latitude, longitude, minradius, maxradius, mindepth, maxdepth, minmagnitude, maxmagnitude, magnitudetype, eventtype, includeallorigins, includeallmagnitudes, includearrivals, eventid, limit, offset, orderby, catalog, contributor, updatedafter, filename, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m setup_query_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m, locs, kwargs)\n\u001b[1;32m    547\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_url_from_parameters(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_PARAMETERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs)\n\u001b[0;32m--> 550\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1486\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[1;32m   1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m download_url(\n\u001b[1;32m   1483\u001b[0m     url, opener\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_opener, headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1484\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug, return_string\u001b[38;5;241m=\u001b[39mreturn_string, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1485\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, use_gzip\u001b[38;5;241m=\u001b[39muse_gzip)\n\u001b[0;32m-> 1486\u001b[0m \u001b[43mraise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy-deploy/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1834\u001b[0m, in \u001b[0;36mraise_on_error\u001b[0;34m(code, data)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;66;03m# No data.\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m204\u001b[39m:\n\u001b[0;32m-> 1834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FDSNNoDataException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data available for request.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1835\u001b[0m                               server_info)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m   1837\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad request. If you think your request was valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1838\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease contact the developers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFDSNNoDataException\u001b[0m: No data available for request.\nHTTP Status code: 204\nDetailed response of server:\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = Client(base_url='https://data.raspberryshake.org')\n",
    "\n",
    "# Set time window\n",
    "start_time = UTCDateTime(START_TIME)\n",
    "end_time = UTCDateTime(END_TIME)\n",
    "\n",
    "print(f\"Time window:\")\n",
    "print(f\"  Start: {start_time}\")\n",
    "print(f\"  End: {end_time}\")\n",
    "print(f\"  Duration: {(end_time - start_time)/60:.1f} minutes\")\n",
    "\n",
    "# Download data\n",
    "print(f\"\\nDownloading data for station {NETWORK}.{STATION}...\")\n",
    "try:\n",
    "    stream = client.get_waveforms(\n",
    "        network=NETWORK,\n",
    "        station=STATION,\n",
    "        location=\"00\",\n",
    "        channel=f\"{CHANNEL[0:2]}*\",\n",
    "        starttime=start_time,\n",
    "        endtime=end_time\n",
    "    )\n",
    "    \n",
    "    # Preprocess\n",
    "    stream.detrend('linear')\n",
    "    stream.detrend('demean')\n",
    "    stream.taper(max_percentage=0.05)\n",
    "    \n",
    "    # Select vertical component\n",
    "    trace = stream.select(channel=CHANNEL)[0]\n",
    "    sampling_rate = trace.stats.sampling_rate\n",
    "    \n",
    "    print(f\"\\n✓ Data downloaded successfully!\")\n",
    "    print(f\"  Station: {trace.stats.station}\")\n",
    "    print(f\"  Channel: {trace.stats.channel}\")\n",
    "    print(f\"  Sampling rate: {sampling_rate} Hz\")\n",
    "    print(f\"  Duration: {(trace.stats.endtime - trace.stats.starttime):.1f} seconds\")\n",
    "    print(f\"  Samples: {len(trace.data)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41d2a5",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent model\n",
    "models_dir = Path(\"../../models\")\n",
    "model_files = sorted(models_dir.glob(\"seismic_cnn_*.pth\"))\n",
    "\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\"No trained model found. Run ../03_training/train_cnn_multiclass.ipynb first.\")\n",
    "\n",
    "model_path = model_files[-1]\n",
    "print(f\"Loading model: {model_path.name}\")\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Extract model configuration\n",
    "model_type = checkpoint['model_type']\n",
    "num_classes = checkpoint['num_classes']\n",
    "input_channels = checkpoint['input_channels']\n",
    "input_length = checkpoint['input_length']\n",
    "class_names = checkpoint['class_names']\n",
    "test_accuracy = checkpoint.get('test_accuracy', 0)\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Type: {model_type}\")\n",
    "print(f\"  Classes: {class_names}\")\n",
    "print(f\"  Input shape: ({input_channels}, {input_length})\")\n",
    "print(f\"  Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Initialize model\n",
    "if model_type == 'standard':\n",
    "    model = SeismicCNN(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_length=input_length\n",
    "    )\n",
    "else:\n",
    "    model = CompactSeismicCNN(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_length=input_length\n",
    "    )\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697cd78",
   "metadata": {},
   "source": [
    "## Window Data and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8214e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing parameters\n",
    "window_samples = int(WINDOW_LENGTH_SEC * sampling_rate)\n",
    "step_samples = int(window_samples * (1 - WINDOW_OVERLAP))\n",
    "\n",
    "# Check if window size matches model\n",
    "if window_samples != input_length:\n",
    "    print(f\"⚠️  Warning: Window size ({window_samples}) doesn't match model input ({input_length})\")\n",
    "    print(f\"    Adjusting window size to match model...\")\n",
    "    window_samples = input_length\n",
    "    WINDOW_LENGTH_SEC = window_samples / sampling_rate\n",
    "\n",
    "# Calculate number of windows\n",
    "n_samples = len(trace.data)\n",
    "n_windows = (n_samples - window_samples) // step_samples + 1\n",
    "\n",
    "print(f\"Windowing configuration:\")\n",
    "print(f\"  Window length: {WINDOW_LENGTH_SEC:.1f} seconds ({window_samples} samples)\")\n",
    "print(f\"  Step size: {step_samples} samples\")\n",
    "print(f\"  Total windows: {n_windows}\")\n",
    "\n",
    "# Extract windows and predict\n",
    "predictions = []\n",
    "probabilities = []\n",
    "window_times = []\n",
    "windowed_data = []\n",
    "\n",
    "print(f\"\\nProcessing windows...\")\n",
    "with torch.no_grad():\n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * step_samples\n",
    "        end_idx = start_idx + window_samples\n",
    "        \n",
    "        if end_idx > n_samples:\n",
    "            break\n",
    "        \n",
    "        # Extract window\n",
    "        window = trace.data[start_idx:end_idx]\n",
    "        \n",
    "        # Normalize\n",
    "        window_normalized = (window - np.mean(window)) / (np.std(window) + 1e-10)\n",
    "        \n",
    "        # Store for visualization\n",
    "        windowed_data.append(window_normalized)\n",
    "        \n",
    "        # Prepare for model: (1, 1, window_length)\n",
    "        window_tensor = torch.FloatTensor(window_normalized).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        output = model(window_tensor)\n",
    "        probs = F.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        pred = np.argmax(probs)\n",
    "        \n",
    "        # Store results\n",
    "        predictions.append(pred)\n",
    "        probabilities.append(probs)\n",
    "        window_time = trace.stats.starttime + (start_idx / sampling_rate)\n",
    "        window_times.append(window_time)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i + 1}/{n_windows} windows...\")\n",
    "\n",
    "# Convert to arrays\n",
    "predictions = np.array(predictions)\n",
    "probabilities = np.array(probabilities)\n",
    "windowed_data = np.array(windowed_data)\n",
    "\n",
    "print(f\"\\n✓ Predictions complete!\")\n",
    "print(f\"  Total windows processed: {len(predictions)}\")\n",
    "\n",
    "# Calculate detection statistics\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(f\"\\nDetection summary:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {class_names[label]}: {count} windows ({count/len(predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821a8a8",
   "metadata": {},
   "source": [
    "## Visualize Classification Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1252a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time axis in seconds from start\n",
    "times_sec = np.array([(t - trace.stats.starttime) for t in window_times])\n",
    "\n",
    "# Define colors for each class\n",
    "colors_map = {0: 'gray', 1: 'orange', 2: 'red'}\n",
    "colors_list = [colors_map[p] for p in predictions]\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "\n",
    "# 1. Raw waveform\n",
    "ax0 = axes[0]\n",
    "time_axis = trace.times()\n",
    "ax0.plot(time_axis, trace.data, 'k-', linewidth=0.5, alpha=0.7)\n",
    "ax0.set_ylabel('Amplitude', fontsize=12)\n",
    "ax0.set_title(f'Raw Seismogram - {NETWORK}.{STATION}.{CHANNEL}', fontsize=14)\n",
    "ax0.grid(True, alpha=0.3)\n",
    "ax0.set_xlim(0, time_axis[-1])\n",
    "\n",
    "# 2. Classification timeline\n",
    "ax1 = axes[1]\n",
    "for i, (time, pred) in enumerate(zip(times_sec, predictions)):\n",
    "    ax1.scatter(time, pred, c=colors_map[pred], s=50, alpha=0.6)\n",
    "ax1.set_ylabel('Predicted Class', fontsize=12)\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax1.set_yticklabels(class_names)\n",
    "ax1.set_title('Classification Timeline', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, times_sec[-1])\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [mpatches.Patch(color=colors_map[i], label=class_names[i]) for i in range(num_classes)]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# 3. Probability for Earthquake class\n",
    "ax2 = axes[2]\n",
    "ax2.fill_between(times_sec, 0, probabilities[:, 2], color='red', alpha=0.3, label='Earthquake')\n",
    "ax2.plot(times_sec, probabilities[:, 2], 'r-', linewidth=1)\n",
    "ax2.axhline(y=CONFIDENCE_THRESHOLD, color='k', linestyle='--', linewidth=1, label=f'Threshold ({CONFIDENCE_THRESHOLD})')\n",
    "ax2.set_ylabel('Probability', fontsize=12)\n",
    "ax2.set_title('Earthquake Detection Probability', fontsize=14)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, times_sec[-1])\n",
    "\n",
    "# 4. Probability for all classes\n",
    "ax3 = axes[3]\n",
    "ax3.plot(times_sec, probabilities[:, 0], color='gray', linewidth=1, label='Noise', alpha=0.7)\n",
    "ax3.plot(times_sec, probabilities[:, 1], color='orange', linewidth=1, label='Traffic', alpha=0.7)\n",
    "ax3.plot(times_sec, probabilities[:, 2], color='red', linewidth=1, label='Earthquake', alpha=0.7)\n",
    "ax3.set_ylabel('Probability', fontsize=12)\n",
    "ax3.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax3.set_title('All Class Probabilities', fontsize=14)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(0, times_sec[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Timeline visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff3a9c",
   "metadata": {},
   "source": [
    "## Visualize Example Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example windows from each class\n",
    "n_rows = 3\n",
    "n_cols = 4\n",
    "n_examples = min(N_EXAMPLE_WINDOWS, n_rows * n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "examples_per_class = n_examples // num_classes\n",
    "plot_idx = 0\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    # Find windows of this class with high confidence\n",
    "    class_mask = predictions == class_idx\n",
    "    class_probs = probabilities[class_mask, class_idx]\n",
    "    class_indices = np.where(class_mask)[0]\n",
    "    \n",
    "    if len(class_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Select highest confidence examples\n",
    "    top_indices = np.argsort(class_probs)[-examples_per_class:]\n",
    "    selected_indices = class_indices[top_indices]\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        if plot_idx >= n_examples:\n",
    "            break\n",
    "            \n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        # Get window data\n",
    "        waveform = windowed_data[idx]\n",
    "        pred_label = predictions[idx]\n",
    "        confidence = probabilities[idx, pred_label]\n",
    "        \n",
    "        # Plot waveform\n",
    "        time_ax = np.arange(len(waveform)) / sampling_rate\n",
    "        ax.plot(time_ax, waveform, color=colors_map[pred_label], linewidth=1)\n",
    "        \n",
    "        # Title with info\n",
    "        title = f\"{class_names[pred_label]}\\n\"\n",
    "        title += f\"Conf: {confidence*100:.1f}% | T={times_sec[idx]:.1f}s\"\n",
    "        ax.set_title(title, fontsize=9, color=colors_map[pred_label])\n",
    "        ax.set_xlabel('Time (s)', fontsize=8)\n",
    "        ax.set_ylabel('Amplitude', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(labelsize=7)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(plot_idx, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Example Detections - {NETWORK}.{STATION}', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Example detections displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314b3f7",
   "metadata": {},
   "source": [
    "## Detection Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate high-confidence detections\n",
    "earthquake_detections = (predictions == 2) & (probabilities[:, 2] > CONFIDENCE_THRESHOLD)\n",
    "traffic_detections = (predictions == 1) & (probabilities[:, 1] > CONFIDENCE_THRESHOLD)\n",
    "\n",
    "n_eq_detections = np.sum(earthquake_detections)\n",
    "n_traffic_detections = np.sum(traffic_detections)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStation: {NETWORK}.{STATION}.{CHANNEL}\")\n",
    "print(f\"Time window: {trace.stats.starttime} to {trace.stats.endtime}\")\n",
    "print(f\"Duration: {(trace.stats.endtime - trace.stats.starttime)/60:.1f} minutes\")\n",
    "print(f\"\\nTotal windows analyzed: {len(predictions)}\")\n",
    "print(f\"Window size: {WINDOW_LENGTH_SEC:.1f} seconds\")\n",
    "print(f\"Overlap: {WINDOW_OVERLAP*100:.0f}%\")\n",
    "\n",
    "print(f\"\\n--- All Predictions ---\")\n",
    "for label in range(num_classes):\n",
    "    count = np.sum(predictions == label)\n",
    "    percentage = count / len(predictions) * 100\n",
    "    print(f\"  {class_names[label]:<12}: {count:4d} windows ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- High Confidence Detections (>{CONFIDENCE_THRESHOLD*100:.0f}%) ---\")\n",
    "print(f\"  Earthquakes: {n_eq_detections} windows\")\n",
    "print(f\"  Traffic:     {n_traffic_detections} windows\")\n",
    "\n",
    "if n_eq_detections > 0:\n",
    "    eq_times = times_sec[earthquake_detections]\n",
    "    eq_confidences = probabilities[earthquake_detections, 2]\n",
    "    print(f\"\\n--- Earthquake Detections ---\")\n",
    "    print(f\"  Time range: {eq_times.min():.1f}s - {eq_times.max():.1f}s\")\n",
    "    print(f\"  Peak confidence: {eq_confidences.max()*100:.1f}%\")\n",
    "    print(f\"  Mean confidence: {eq_confidences.mean()*100:.1f}%\")\n",
    "    \n",
    "    # Find continuous earthquake segments\n",
    "    eq_indices = np.where(earthquake_detections)[0]\n",
    "    gaps = np.diff(eq_indices)\n",
    "    segment_starts = [eq_indices[0]]\n",
    "    segment_ends = []\n",
    "    \n",
    "    for i, gap in enumerate(gaps):\n",
    "        if gap > 2:  # Gap of more than 2 windows\n",
    "            segment_ends.append(eq_indices[i])\n",
    "            segment_starts.append(eq_indices[i+1])\n",
    "    segment_ends.append(eq_indices[-1])\n",
    "    \n",
    "    print(f\"\\n  Detected {len(segment_starts)} earthquake segment(s):\")\n",
    "    for i, (start, end) in enumerate(zip(segment_starts, segment_ends)):\n",
    "        duration = (times_sec[end] - times_sec[start]) + WINDOW_LENGTH_SEC\n",
    "        print(f\"    Segment {i+1}: {times_sec[start]:.1f}s - {times_sec[end]:.1f}s (duration: {duration:.1f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33bf7b",
   "metadata": {},
   "source": [
    "## Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'window_id': np.arange(len(predictions)),\n",
    "    'start_time': [str(t) for t in window_times],\n",
    "    'time_sec': times_sec,\n",
    "    'prediction': predictions,\n",
    "    'predicted_class': [class_names[p] for p in predictions],\n",
    "    'prob_noise': probabilities[:, 0],\n",
    "    'prob_traffic': probabilities[:, 1],\n",
    "    'prob_earthquake': probabilities[:, 2],\n",
    "    'confidence': np.max(probabilities, axis=1)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = Path(\"predictions\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f\"predictions_{STATION}_{timestamp}.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")\n",
    "print(f\"  Total rows: {len(results_df)}\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(results_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
